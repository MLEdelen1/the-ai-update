<!doctype html><html lang="en"><head><meta charset="UTF-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><title>Use Ollama Guide</title><link rel="stylesheet" href="../styles.css?v=14" /></head><body><canvas id="particles"></canvas><div class="top-glow"></div><nav id="nav"><div class="nav-wrap"><a href="../index.html" class="logo-group"><img src="../assets/logo.png" class="logo-img" alt=""><div class="logo-text">THE AI<span class="logo-accent">UPDATE</span></div></a><div class="nav-links" id="navLinks"><a href="../intel.html">Latest Intel</a><a href="../workflows.html">Workflows</a><a href="../tools.html">Tools</a><a href="index.html">Guides</a><a href="../articles.html">Articles</a><a href="../resources.html">Resources</a><a href="../starter-kit.html" class="nav-btn">Get Free Kit</a></div><button class="burger" onclick="document.getElementById('navLinks').classList.toggle('show')"><span></span><span></span><span></span></button></div></nav><main class="contain article-body" style="padding-top:100px"><div class="sect-head"><p class="tag">USE GUIDE â€” OLLAMA</p><h1>Main takeaway: start local chat first, then test API, then automate</h1><p class="sect-sub">API means application programming interface, a URL apps use to exchange data with your local model.</p></div><section class="intel-grid"><article class="intel-card"><div class="intel-source">WINDOWS</div><h3>Model check</h3><pre><code>ollama list
ollama run llama3.2:3b "Say hello in one short sentence."</code></pre><p><strong>Expected result:</strong> model list appears and model responds in one sentence.</p></article><article class="intel-card"><div class="intel-source">macOS</div><h3>Model check</h3><pre><code>ollama list
ollama run llama3.2:3b "Say hello in one short sentence."</code></pre><p><strong>Expected result:</strong> same output pattern as Windows.</p></article><article class="intel-card"><div class="intel-source">Linux</div><h3>Model check</h3><pre><code>ollama list
ollama run llama3.2:3b "Say hello in one short sentence."</code></pre><p><strong>Expected result:</strong> local model answers directly in terminal.</p></article><article class="intel-card"><div class="intel-source">API TEST</div><h3>Call localhost API</h3><pre><code>curl http://localhost:11434/api/generate -d '{"model":"llama3.2:3b","prompt":"Give one productivity tip.","stream":false}'</code></pre><p><strong>Expected result:</strong> JSON response that includes generated text.</p></article><article class="intel-card"><div class="intel-source">PRACTICAL USE CASE</div><h3>Private knowledge draft assistant</h3><p>Feed sanitized internal notes to a local model, generate draft replies, and keep sensitive details on your own machine. This is ideal for support teams, operations notes, and repetitive report writing.</p></article><article class="intel-card"><div class="intel-source">TROUBLESHOOTING</div><h3>Common fixes</h3><p>If <code>ollama list</code> fails, start server with <code>ollama serve</code>.</p><p>If model is missing, run <code>ollama pull llama3.2:3b</code>.</p><p>If port 11434 fails, restart Ollama service and verify no firewall block.</p></article></section><section class="tools-row"><article class="tool"><div class="tool-cat">IF NEEDED</div><h3>Install first</h3><p>Use setup guide if Ollama is not installed yet.</p><a class="tool-link" href="ollama-setup.html">Open setup guide &rarr;</a></article><article class="tool"><div class="tool-cat">NEXT</div><h3>Combine tools</h3><p>Wire Ollama into n8n and OpenClaw for reusable automation.</p><a class="tool-link" href="openclaw-n8n-ollama-workflow.html">Open combine guide &rarr;</a></article></section></main><footer><div class="contain foot-inner"><div class="foot-brand"><img src="../assets/logo.png" class="foot-logo" alt=""><span>THE AI UPDATE</span></div><div class="foot-links"><a href="../index.html">Home</a><a href="index.html">Guides</a><a href="../articles.html">Articles</a><a href="../tools.html">Tools</a><a href="../resources.html">Resources</a><a href="../starter-kit.html">Starter Kit</a></div></div></footer><script src="../site.js?v=14"></script></body></html>