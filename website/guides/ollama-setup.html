<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Ollama Setup Guide</title>
  <link rel="stylesheet" href="../styles.css?v=9" />
</head>
<body>
  <canvas id="particles"></canvas><div class="top-glow"></div>
  <nav id="nav"><div class="nav-wrap"><a href="../index.html" class="logo-group"><img src="../assets/logo.png" class="logo-img" alt=""><div class="logo-text">THE AI<span class="logo-accent">UPDATE</span></div></a><div class="nav-links" id="navLinks"><a href="../intel.html">Latest Intel</a><a href="../workflows.html">Workflows</a><a href="../tools.html">Tools</a><a href="../guides/local-ai-stack.html">Guides</a><a href="../articles.html">Articles</a><a href="../resources.html">Resources</a><a href="../starter-kit.html" class="nav-btn">Get Free Kit</a></div><button class="burger" onclick="document.getElementById('navLinks').classList.toggle('show')"><span></span><span></span><span></span></button></div></nav>
  <main class="contain article-body" style="padding-top:100px">
    <div class="sect-head"><p class="tag">GUIDE â€¢ OLLAMA</p><h1>Ollama setup for private local model inference</h1><p class="sect-sub">Install Ollama, pull models, run repeatable tests, and diagnose common GPU, memory, and networking failures.</p></div>
    
<section class="intel-grid">
<article class="intel-card"><div class="intel-source">STEP 1</div><h3>Install Ollama</h3><h4>Windows PowerShell</h4><pre><code>winget install Ollama.Ollama
ollama --version</code></pre><h4>macOS/Linux (optional)</h4><pre><code>curl -fsSL https://ollama.com/install.sh | sh
ollama --version</code></pre></article>
<article class="intel-card"><div class="intel-source">STEP 2</div><h3>Start service and pull starter models</h3><p>Use one small, one medium model for comparison.</p><pre><code>ollama serve
# in a second terminal
ollama pull llama3.2:3b
ollama pull qwen2.5:7b</code></pre></article>
<article class="intel-card"><div class="intel-source">STEP 3</div><h3>Run benchmark prompts</h3><p>Evaluate consistency, speed, and factual behavior with the same exact prompt set.</p><pre><code>ollama run llama3.2:3b "Summarize this incident in 5 bullets and one action plan."
ollama run qwen2.5:7b "Summarize this incident in 5 bullets and one action plan."
ollama list</code></pre></article>
<article class="intel-card"><div class="intel-source">STEP 4</div><h3>Verify API endpoint locally</h3><pre><code>curl http://localhost:11434/api/tags
curl http://localhost:11434/api/generate -d '{"model":"llama3.2:3b","prompt":"Say hello in one sentence."}'</code></pre></article>
<article class="intel-card"><div class="intel-source">VERIFY</div><h3>Verification commands</h3><pre><code>ollama --version
ollama list
curl http://localhost:11434/api/tags
Get-NetTCPConnection -LocalPort 11434 -ErrorAction SilentlyContinue</code></pre></article>
<article class="intel-card"><div class="intel-source">TROUBLESHOOT</div><h3>Troubleshooting commands</h3><pre><code># restart service
Stop-Process -Name ollama -Force -ErrorAction SilentlyContinue
Start-Process ollama -ArgumentList "serve"

# clear broken model pull (if needed)
ollama rm llama3.2:3b
ollama pull llama3.2:3b

# check GPU/driver in Windows
nvidia-smi

# fallback to CPU test prompt
ollama run llama3.2:3b "CPU fallback test"</code></pre></article>
</section>
<p style="margin:28px 0"><a class="btn-main" href="local-ai-stack.html">Continue to full stack roadmap</a></p>
  </main>
  <footer><div class="contain foot-inner"><div class="foot-brand"><img src="../assets/logo.png" class="foot-logo" alt=""><span>THE AI UPDATE</span></div><div class="foot-links"><a href="../index.html">Home</a><a href="../guides/local-ai-stack.html">Guides</a><a href="../articles.html">Articles</a><a href="../tools.html">Tools</a><a href="../resources.html">Resources</a><a href="../starter-kit.html">Starter Kit</a></div></div></footer>
  <script src="../site.js"></script>
</body>
</html>